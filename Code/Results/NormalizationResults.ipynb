{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9978819c-abbd-450b-a6cc-d049b96461d1",
   "metadata": {},
   "source": [
    "# Results for Generalization using KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b61c11e-58a4-483f-8926-db93ad6787f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1e6251-6f7e-4d9e-83c1-629a8259b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probability distribution files for weak generalization - Standard set transformer\n",
    "wg_pt_path1 = './probability_output_files/Weak_Generalization/64_16_128_3_0.05/'\n",
    "wg_pt_output_files1 = [file for file in os.listdir(wg_pt_path1) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for strong generalization - Standard set transformer\n",
    "sg_pt_path1 = './probability_output_files/Strong_Generalization/64_16_128_3_0.05/'\n",
    "sg_pt_output_files1 = [file for file in os.listdir(sg_pt_path1) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for weak generalization - contrastive pre-trained model\n",
    "wg_pt_path2 = './probability_output_files/Weak_Generalization/masked_encoder_64_16_128_3_0.05/'\n",
    "wg_pt_output_files2 = [file for file in os.listdir(wg_pt_path2) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for strong generalization - contrastive pre-trained model\n",
    "sg_pt_path2 = './probability_output_files/Strong_Generalization/masked_encoder_64_16_128_3_0.05/'\n",
    "sg_pt_output_files2 = [file for file in os.listdir(sg_pt_path2) if file.startswith('output_file_') and file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e4c64d-9feb-4ce7-8449-949051fb3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(path, output_files):\n",
    "    df_dict = {}\n",
    "    for i, file in enumerate(output_files):\n",
    "        df_dict[f'df_{i+1}'] = pd.read_csv(path+file)\n",
    "        df_dict[f'df_{i+1}'].columns = df_dict[f'df_{i+1}'].columns.astype(int)\n",
    "        \n",
    "    return df_dict\n",
    "    \n",
    "wg_pt_df_dict1 = concat(wg_pt_path1, wg_pt_output_files1)\n",
    "sg_pt_df_dict1 = concat(sg_pt_path1, sg_pt_output_files1)\n",
    "wg_pt_df_dict2 = concat(wg_pt_path2, wg_pt_output_files2)\n",
    "sg_pt_df_dict2 = concat(sg_pt_path2, sg_pt_output_files2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a8aa6e-f69a-4019-a681-55e6592ef0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D matrix from the DataFrames\n",
    "wg_pt_data_matrix1 = np.array([df1.values for df1 in wg_pt_df_dict1.values()])\n",
    "sg_pt_data_matrix1 = np.array([df1.values for df1 in sg_pt_df_dict1.values()])\n",
    "wg_pt_data_matrix2 = np.array([df1.values for df1 in wg_pt_df_dict2.values()])\n",
    "sg_pt_data_matrix2 = np.array([df1.values for df1 in sg_pt_df_dict2.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4d8ee9-cd91-4be4-8bd7-16fe141fb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 75, 75)\n",
      "(10, 75, 75)\n",
      "(10, 75, 75)\n",
      "(10, 75, 75)\n"
     ]
    }
   ],
   "source": [
    "print(wg_pt_data_matrix1.shape)\n",
    "print(sg_pt_data_matrix1.shape)\n",
    "print(wg_pt_data_matrix2.shape)\n",
    "print(sg_pt_data_matrix2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bc9dfb-f11d-4437-b30e-b31de4282734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean along the first axis (axis=0)\n",
    "# To combine all individual run\n",
    "wg_pt_combined_matrix1 = np.mean(wg_pt_data_matrix1, axis=0)\n",
    "sg_pt_combined_matrix1 = np.mean(sg_pt_data_matrix1, axis=0)\n",
    "wg_pt_combined_matrix2 = np.mean(wg_pt_data_matrix2, axis=0)\n",
    "sg_pt_combined_matrix2 = np.mean(sg_pt_data_matrix2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e227dd8-b58c-4b73-a8c2-e60b4fbb68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing diagonal values with small number for normalization\n",
    "np.fill_diagonal(wg_pt_combined_matrix1, 1e-9)\n",
    "np.fill_diagonal(wg_pt_combined_matrix2, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23462ebf-9784-4023-b6a5-4ee3c77394da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.994048784534293\n",
      "75.00000055265839\n",
      "3.655458825502211\n",
      "75.00000047316911\n"
     ]
    }
   ],
   "source": [
    "# Get sum for normalization\n",
    "wg_pt_matrix_sum1 = pd.DataFrame(wg_pt_combined_matrix1).sum().sum()\n",
    "print(wg_pt_matrix_sum1)\n",
    "sg_pt_matrix_sum1 = pd.DataFrame(sg_pt_combined_matrix1).sum().sum()\n",
    "print(sg_pt_matrix_sum1)\n",
    "\n",
    "wg_pt_matrix_sum2 = pd.DataFrame(wg_pt_combined_matrix2).sum().sum()\n",
    "print(wg_pt_matrix_sum2)\n",
    "sg_pt_matrix_sum2 = pd.DataFrame(sg_pt_combined_matrix2).sum().sum()\n",
    "print(sg_pt_matrix_sum2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3541f8-56f6-4400-bc56-f83550bbf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Normalization\n",
    "wg_pt_mean_matrix_partial1 = wg_pt_combined_matrix1/wg_pt_matrix_sum1\n",
    "sg_pt_mean_matrix_partial1 = sg_pt_combined_matrix1/sg_pt_matrix_sum1\n",
    "\n",
    "wg_pt_mean_matrix_partial2 = wg_pt_combined_matrix2/wg_pt_matrix_sum2\n",
    "sg_pt_mean_matrix_partial2 = sg_pt_combined_matrix2/sg_pt_matrix_sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de2c6de-6649-40ee-ba38-3a8b5d4a0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten for KL Divergence\n",
    "wg_pt_mean_matrix_partial_flatten1 = wg_pt_mean_matrix_partial1.flatten()\n",
    "sg_pt_mean_matrix_partial_flatten1 = sg_pt_mean_matrix_partial1.flatten()\n",
    "\n",
    "wg_pt_mean_matrix_partial_flatten2 = wg_pt_mean_matrix_partial2.flatten()\n",
    "sg_pt_mean_matrix_partial_flatten2 = sg_pt_mean_matrix_partial2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731f9833-8cef-4581-8d86-0ee85d75ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there is no 0, as KL divergence will be NAN\n",
    "wg_pt_mean_matrix_partial_flatten1[wg_pt_mean_matrix_partial_flatten1 < 1e-15] = 1e-9\n",
    "sg_pt_mean_matrix_partial_flatten1[sg_pt_mean_matrix_partial_flatten1 < 1e-15] = 1e-9\n",
    "\n",
    "wg_pt_mean_matrix_partial_flatten2[wg_pt_mean_matrix_partial_flatten2 < 1e-15] = 1e-9\n",
    "sg_pt_mean_matrix_partial_flatten2[sg_pt_mean_matrix_partial_flatten2 < 1e-15] = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880fae05-62e7-4fbf-8d27-44c424507086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate KL divergence\n",
    "def kl_divergence(p, q, epsilon=1e-9):\n",
    "    return np.sum(p * np.log(p / q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e7b7efb-1985-401a-940e-8053e27e1838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 6.60602694038697\n",
      "KL Divergence: 6.254128400859809\n"
     ]
    }
   ],
   "source": [
    "# Calculate KL divergence - Standard Set Transformer\n",
    "kl_divergence_value1 = np.sum([kl_divergence(p_row, q_row) for p_row, q_row in zip(wg_pt_mean_matrix_partial_flatten1, sg_pt_mean_matrix_partial_flatten1)])\n",
    "print(\"KL Divergence:\", kl_divergence_value1)\n",
    "\n",
    "# Calculate KL divergence - Contrastive Pre-trained Model\n",
    "kl_divergence_value2 = np.sum([kl_divergence(p_row, q_row) for p_row, q_row in zip(wg_pt_mean_matrix_partial_flatten2, sg_pt_mean_matrix_partial_flatten2)])\n",
    "print(\"KL Divergence:\", kl_divergence_value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb2104a-1a59-4b48-b798-ebcfd0470c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall KL Divergence: 9.53048244104579\n",
      "Overall KL Divergence: 9.022794066617676\n"
     ]
    }
   ],
   "source": [
    "# KL Divergence as suggested by the chatGPT usin gentropy\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Calculate KL divergence between the two distributions\n",
    "kl_divergence_array1 = entropy(wg_pt_mean_matrix_partial_flatten1, sg_pt_mean_matrix_partial_flatten1, base=2)\n",
    "\n",
    "kl_divergence_value1 = np.mean(kl_divergence_array1)  # or np.sum(kl_divergence_array)\n",
    "print(\"Overall KL Divergence:\", kl_divergence_value1)\n",
    "\n",
    "kl_divergence_array2 = entropy(wg_pt_mean_matrix_partial_flatten2, sg_pt_mean_matrix_partial_flatten2, base=2)\n",
    "\n",
    "kl_divergence_value2 = np.mean(kl_divergence_array2)  # or np.sum(kl_divergence_array)\n",
    "print(\"Overall KL Divergence:\", kl_divergence_value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3df0c-5bf7-47eb-99bd-50f8ec1a6c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
